Goals for the next three days:

1 Read Dan's article and understand the idea of clustering embeddings
2 Try implementing BERTopic to cluster all the relevant topics about an enterprise
3 See how other resources have done sentiment analysis on twitter before you to get a sense of that
4 Read amirmokhtar article (article about sentiment analysis in twitter with very inetresting approach on scrapping and analysis)
5 Try different embeddings with the Kmeans
6 Do some research about database
7 Get a sense of what FLASK is
8 Research preprocessing


Topic modeling vs clustering:

The purpose of topic modeling methods is to discover the latent themes (topics) assumed to have generated the documents of a 
corpus. Topic modeling methods are built on the distributional hypothesis, suggesting that similar words occur in similar 
contexts. To this end, they assume a generative process (a sequence of steps), which is a set of assumptions that describe 
how the documents are generated. Given the assumptions of the generative process, inference is done, which results in 
learning the latent variables of the model. For instance, for Latent Dirichlet Allocation, this is the per-topic document 
distributions and the per-word document distributions. In this sense, a document can be represented by its per-topic 
distribution (doc1 = 0.3×Sports + 0.7×Cinema). This later can be seen as a soft clustering approach, i.e., doc1 belongs 30% 
in cluster Sports and 70% in Cinema. But topic models are not solely clustering methods, as can also been used for 
understanding, exploring, visualizing a collection.

On the other hand, clustering methods aim at partitioning data into coherent groups. Of course, what is coherent and how 
the partitioning is performed differs between the various clustering algorithms. The distance between the data instances 
is central for clustering methods and for this the instances can be represented in various ways: for documents this can 
be term frequencies (tf), tf-idf, and even with the per-document topic distributions learned by topic models.



Part 1: Read Dan's article and understand the idea of clustering embeddings

Dan's article is quite complicated, to be reviewed later since it's just an alternative to K-means
Good introduction to embeddings:
https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526
BERT's embedding has a better performance than word2vec which is limited
I added a line that extracts the embedding from model in the sent_analysis.ipynb


Part 2: Try implementing BERTopic to cluster all the relevant topics about an enterprise

The local implementation is not working for some packages issues, thus i'll try doing it on colab
This is a great reference explaining the BERTopic function and its implementation
https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing#scrollTo=IVpvT4bA6KiN
To be reviewed tomorrow how to change embedding in BERTopic:
https://colab.research.google.com/drive/18arPPe50szvcCp_Y6xS56H2tY0m-RLqv?usp=sharing
For now we have a rather simple implementation of BERTopic in our repository


Part 3: See how other resources have done sentiment analysis on twitter before you to get a sense of that

Explaining the whole process, it's a good start, to be checked later today
https://monkeylearn.com/blog/sentiment-analysis-of-twitter/
Basically it gave us the whole steps that were discussed earlier (preprocessing until sentiment analysis)
Twitter’s way of talking is a lot more informal and specific than generic datasets. There is also the different 
sentiment in specific words that differs from the mean; examples of the latter are: bull, sell, buy…
Explains basically the same old procedure 
https://medium.com/@erik.terres.es/fine-tuning-bert-for-sentiment-analysis-on-non-labeled-twitter-data-1fd8951fbbd4

Using word2vec k-means in order to cluster sentiment between positive and negative.
https://towardsdatascience.com/unsupervised-sentiment-analysis-a38bf1906483
https://github.com/rafaljanwojcik/Unsupervised-Sentiment-Analysis
His repository is cloned
The file word2vec in code is a implementation of the process explained above. It failed to give interesting results 
though the git repository was promising.


Part 4 Read amirmokhtar article

Which aspects of sustainability do the consumers of cell phones often tweet?

Three procedures from various intellectual foun- dations, which are entitled as Descriptive analysis
After collecting the initial data using Twitter search APIs, they used statistical analysis, text mining, and Sentiment 
analysis methods to analyze the data taken from this social me- dia as well as showing the relevant patterns and information 
and understanding the customers’ perceptions.
The outcome was that as we get more data from the Streaming API, the contextual analysis is the most reliable.
In conclusion:
A very useful document whith a lot of interesting references.
To be reviewed later when implementing the scraping efficiently
+ there's a reference chae_2015 to be reviewed for that same purpose


Part 5: Try different embeddings with the Topic modeling

A very nice recapitulatif on embeddings:
https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc


Part 6: Preprocessing is a priority ( do some research in your pipeline from scratch)


Part 6 Do some research about database

Dataset about climate change sentiment
https://www.kaggle.com/datasets/deffro/the-climate-change-twitter-dataset?select=disasters.csv
