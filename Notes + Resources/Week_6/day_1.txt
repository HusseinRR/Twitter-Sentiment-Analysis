Idea: Do supervised Umap training on the dictionnary of E,S and G to see what results we get
study tsne vs umap


Lowering the embedding dimensionalities is optional but can save runtime (more details below).
BERTopic is a good option or not?

Reducing dimensionality before clustering has a negligible impact on performance

Steps:
Multiple embeddings extraction
2 or 3 methods max: UMAP + TFIDF, or the paper in the resources
https://towardsdatascience.com/clustering-contextual-embeddings-for-topic-model-1fb15c45b1bd
https://huggingface.co/princeton-nlp/unsup-simcse-bert-base-uncased?text=i+would+like+some+food+please
https://github.com/hyintell/topicx

Calculer la matrice de distance
Db scan est plus intéressant que 

yOU COULD CREATE YOUR OWN embeddings and feed it to BERTopic
Use the guided topic modeling in BERTopic using the dictionnaries we have at hand

Try semi-supervised learning

https://medium.com/tech-that-works/maximal-marginal-relevance-to-rerank-results-in-unsupervised-keyphrase-extraction-22d95015c7c5
An idea would be to look into hashtags 
Try 
CLS or Mean model in huggingface

Use Flask
Ressortir les tweets par topic
FNSEA
Reduire le nombre des clusters -> Les

Sequence output is the sequence of hidden-states (embeddings) at the output of the last layer of the BERT model. It includes the embedding 
of the [CLS] token. Hence, for the sentence "You are on Stackoverflow", it gives 5 embeddings: one embedding for each of the four words 
(assuming the word "Stackoverflow" was tokenized into a single token) along with the embedding of the [CLS] token. Pooled output is the 
embedding of the [CLS] token (from Sequence output), further processed by a Linear layer and a Tanh activation function. The Linear layer 
weights are trained from the next sentence prediction (classification) objective during pretraining. For further details, please refer to 
the BERT original paper.

Create a file for clustering models
Fais un schéma expliaquant les différentes étapes et difficulté en terme de mémoire pour FLASK

Modify preprocessing (retweets done, urls done)

Scrap the tweets done
Save the embeddings each time done
Modify and optimize the models to be seen
See how you can get closest tweets first done for kmeans, 
Semi-supervised: check Gram-schimdt-
mmr  
hashtags 
TD-IDF instead of TF-IDF-idi
https://theblue.ai/services/natural-languageprocessing/topic-modeling-python-gensim/

a document has
high marginal relevance if it is both relevant to the query
and contains minimal similarity to previously selected
documents. We strive to maximize-marginal relevance in
retrieval and summarization, hence we label our method
“maximal marginal relevanci” (MMR).


Left to do:

hashtags find a way to appear the most relevant
Proper integration of GramSchmidt
Find center HdbScan



The clusters may be non-convex, and if you compute the average of all points 
(and your data are points - they don't need to be) it may then be outside of the cluster
+ Computational effort ...
aSK ABOUT HIEREARICHAL DENDROGRAM


Filtrage ESG
Trouver le meilleur k elbow method / ToMATo
Note sentiment analysis
Affichage

Problem with ToMATo: Too many clusters, we would also need to specify the number of neighbors so it's not really a shortcut
Probelm wiith KMeans: the printed images

Eliminate topic if all keywords are not ESG
https://www.kaggle.com/code/rowhitswami/keywords-extraction-using-tf-idf-method/notebook
TFIDF alternative KeyBERT, try them both
https://github.com/flairNLP/flair a nice library allowing to extract embeddings using any hugging face model

Take the most recursive hashtags 


Finalize the Algorithm:
Fix sentiment analysis done
Set a button to remove UMAP done
Fix KMeans elbow done
Add the third esg filter done
See what's the issue with the first embedder (leave to after scraping)

Add scraping:
Comment stockage des données
idea specify the date of scraping, then if you launch the scraper again it will add the new database to the newly made
What you could do: arrange the date of you database by 

Passation:
Documentation
Implement some unit testing

BackEnd to Front:
How to demonstrate to client
Go online?

Writing:
Write article on medium: Use tweets from guardian environment
Rapport de stage 
Aperçu twitter how much to pay ...

Optimize results:
Add hashtags weights
Preprocessing extraction of hashtags add removal of enterprise name 
Optimize Gram-Schmidt
Translation in FinBERT
Reduce the number of files (3 models for embedder put them in one file maybe?)

Further optimizations:
Find an optimal way to scrap tweets
Read articles about translation

Benchmark: done
https://developer.twitter.com/en/docs/twitter-api/tweets/search/introduction
https://developer.twitter.com/en/pricing/search-fullarchive


This week focus:
Transition Back to Front (DB, faire que de clustering)
How to show the client the interface (To discuss with Elodie)
Documentation
Unit testing
Article + Rapport de stage