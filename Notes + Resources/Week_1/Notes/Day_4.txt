Goals:
1 Finish the youtube videos & Try implementing the youtube code
2 Research Ordinal Categorical Classification (ehen two classes are close to each other)
3 Read Freezing_layers_RoBERTa
4 Research on finetuning XLNet
5 Research text clustering if it's interesting for our model
6 Research Topic modeling with sentiment analysis
7 Prepare a presentation of this week's findings


Part 1: Finish the youtube videos & Try implementing the youtube code

We cloned the repository for the finetuning of bert_case. The sentiment_analysis_with_Bert notebook provides a good
struture to adopt for the work.


Part 2: Research Ordinal Categorical Classification (when two classes are close to each other)

https://towardsdatascience.com/simple-trick-to-train-an-ordinal-regression-with-any-classifier-6911183d2a3c
The idea: 
Instead of having a k-classifier we use k binary classifiers each prediction the prbability of being superior to a 
certain scale (form 1 to 5 for example)
We finally use the rule:
P(y=x) = P(y>=x) - P(y>=x+1)
Could be interesting to implement in our model 


Part 3: Read Freezing_layers_RoBERTa

Main Takeayways:
1 Bottom layers attend broadly, while the top layers capture linguistic syntax.
2 a batch size of 16 and fine-tune BERT for 3 epochs and RoBERTa for 10, following the original papers.
3 For hyperparameter tuning, the best learning rate is different for each task, and all of the original authors choose 
one between 1e-5 and 5e-5 ; thus, we perform line search over the interval with a step size of 1e-5.
4 We find that only a fourth of the layers necessarily need to be fine-tuned to obtain 90% of the original quality.

Pratcial implementation:
https://gist.github.com/raphael0202/85580b29b27a27ddaae8d393f686f891#file-train-py-L97


Part 4: Finetuning XLNet:

A basic code to finetune XLNet
https://snrspeaks.medium.com/fine-tuning-xlnet-model-for-text-classification-in-3-lines-of-code-1a7c3b320669
The RoBERTa paper mentioned that the same process could be applied to XLNet


Part 5: Research topic modelling if it's interesting for our work

BERTopic: 
https://github.com/MaartenGr/BERTopic
https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing
It is capable of fitting articles into categories, i would it may be worth a try considering it.
It answers the question ‘Which topic can frequently be found in these documents?’

Explaining how to create topic identifier
https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6
Steps: find data -> pass that data in encoder (for that use sentence transformers: https://github.com/UKPLab/sentence-transformers) 
        -> reduce dimension (using umap for example)-> cluster the outcome (using HDBSAN for example)
Idea: Scrap tweets about a firm from twitter then cluster them usinf BERTopic

An alternative for BERTopic is Top2vec
BERTopic vs Top2Vec: https://github.com/MaartenGr/BERTopic/issues/372
Top2Vec (topic to vector):
Implementation:
https://www.kaggle.com/code/dangelov/covid-19-top2vec-interactive-search
Research paper to consider if we decide to go with Top2Vec: https://arxiv.org/pdf/2008.09470.pdf


Part 6: Research Topic modeling with sentiment analysis

Topic modeling will be used to determine the topics of discussion on Twitter about ESG. Sentiment analysis will be used to 
determine the different levels of positive and negative opinion on the enterprise present in the dataset.
We take an enterprise for example and we try to find all the tweets that are relevant to it. We do the topic modeling on
the collected tweets, then we find which among those topics are interesting for the ESG ratings. We then classify using a sentiment
analysis model on a scale from 1 to 100 in order to determine the ESG score.
This approach has been used before.
