Tasks:

1 Read Alex's articles
2 Try to find an implementation based on hashtags and  cleaned text
3 Improve tokenization (example pyramid scheme should be one word not two, year old)
4 Find a hyperparameter above which the tweet is classified as none
5 Research finetuning
6 A future step to consider is to access the url and evaluate the content it represents
 

Part 1: Read Alex's articles

https://blog.ml6.eu/decoding-sentence-encoders-37e63244ae00
We compared sentence transformers to regular transformers and discovered that one is not universally better than the 
other. Sentence transformers are just tuned for semantic similarity. It is a matter of the right tool for the right job.
We discussed cross-encoders which approach semantic similarity as a classification problem. We concluded that while 
cross-encoders are often very accurate, they are also often not very practical in a real-life setting.
We saw how bi-encoders are designed to offer a solution to the inefficiency of cross-encoders.
We rounded off by discussing the bi-encoder training process which is what really makes it stand out as their underlying 
architecture is identical to that of a regular transformer.


Part 2: Improve tokenization & preprocessing
Found nltk tweet tokenizer specific for the task.
I tried other tokenizers but they all give not too releavant results, none of them is capable of regrouping words in an 
understandable way.
https://pypi.org/project/pyspellchecker/
good library checks misspelling


Part 3: Try to find an implementation based on hashtags and  cleaned text

Either focus on cleaned text as a whole or just the keywords
I think we only need keywords to classify a sentence if it relates to ESG or not
We could then use the cleaned text for sentiment analysis
That's the approach i'll probably use for now
An idea that comes to mind is to create the space generated by the three vectors E, S & G and then you take the orthogonal
to that space.
The dimension of our space is 768


Part 4: Find a hyperparameter above which the tweet is classified as none



Part 5: Finetuning

https://towardsdatascience.com/sentence-transformer-fine-tuning-setfit-outperforms-gpt-3-on-few-shot-text-classification-while-d9a3788f0b4e




